{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Purpose\n",
    "This project demonstrates the process of creating, saving, loading, and updating a vector store using embeddings created from text files in a corpus. It also showcases how to perform text replacement within a file, update the corresponding vector in the vector store, and verify the updates, with all operations now executed in a more granular and modular code structure.\n",
    "\n",
    "## Components\n",
    "1. **FlexiAI Initialization**: Initialize the FlexiAI client and configure logging.\n",
    "2. **Corpus Handling**: Read and manage text files from a specified directory.\n",
    "3. **Embedding Management**: Generate, save, load, and map embeddings for the corpus.\n",
    "4. **Text Processing**: Perform various text-based operations, including similarity search, clustering, semantic search, and question answering.\n",
    "5. **Vector Store Update**: Replace specific text in a file, update the corresponding vector in the vector store, and verify the updates.\n",
    "6. **Result Truncation**: Ensure output remains concise by truncating long text outputs for readability.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "### Step 1: Initialization\n",
    "- **Setup Logging**: Configure logging for tracking the process, with customizable levels for root, file, and console.\n",
    "- **Initialize FlexiAI**: Create an instance of the `FlexiAI` class, initializing various managers including the `LocalVectorStoreManager`.\n",
    "\n",
    "### Step 2: Corpus Handling\n",
    "- **Read Corpus**: Use the `read_corpus_from_directory` method to read all text files from the specified directory and store their contents and file paths.\n",
    "\n",
    "### Step 3: Embedding Management\n",
    "- **Create and Save Embeddings**: Generate embeddings for the read text files using the `create_embeddings_for_faiss` method, and save these embeddings into a FAISS index along with corresponding metadata.\n",
    "- **Load and Map Vector Store**: Load the previously saved FAISS index and metadata using the `load_vector_store` method, and map the vector store for further operations.\n",
    "\n",
    "### Step 4: Text Processing\n",
    "- **Text Similarity Search**: Calculate the similarity between an input text and all texts in the corpus, then identify and print the most similar text. The output is truncated to 300 characters for readability.\n",
    "- **Clustering Texts**: Cluster the texts into a specified number of clusters and print out each cluster. Each text in the clusters is truncated to 300 characters.\n",
    "- **Semantic Search**: Perform a semantic search for a query and find the most relevant document in the corpus. The result is truncated to 300 characters.\n",
    "- **Question Answering**: Find the sentence in the corpus that is most similar to a given question, and print the answer. The output is truncated to 300 characters. For this you need to work more on preprocessing and the system, but you can use a simple way if you want, retrieve the sentence or text and give it to AI to process it in the run and will answer to your question with your data.\n",
    "-> reinforce him with knowledge ;)\n",
    "\n",
    "### Step 5: Vector Store Update\n",
    "- **Replace Text in File**: Define the target file and text to be replaced. Use the `replace_text_in_file_and_update_vector_store` method to perform the replacement and update the corresponding vector in the vector store.\n",
    "- **Save Updated Vector Store**: Save the updated vector store to a specified location.\n",
    "\n",
    "### Step 6: Search and Verification\n",
    "- **Search for Old Text**: Use the `search_for_text_in_vector_store` method to search for the old text in the updated vector store.\n",
    "- **Verify Replacement**: Check if the old text is still present in the vector store and print the result to confirm successful replacement.\n",
    "\n",
    "## Use Cases\n",
    "1. **Reading Text Files**: Ensure that all text files in a specified directory are correctly read and stored.\n",
    "2. **Creating and Saving Embeddings**: Verify that embeddings are correctly created for the text files and saved into a FAISS index.\n",
    "3. **Loading and Mapping Vector Store**: Test the ability to load a previously saved FAISS index and metadata, and map the vector store for further operations.\n",
    "4. **Text Similarity and Clustering**: Validate the functionality to perform text similarity searches and clustering, ensuring that results are truncated for readability.\n",
    "5. **Semantic Search and Question Answering**: Ensure the system can perform semantic searches and answer questions based on the corpus, with outputs truncated for readability.\n",
    "6. **Text Replacement in File**: Validate the ability to replace specific text within a file and update the corresponding vector in the vector store.\n",
    "7. **Searching and Verifying Text in Vector Store**: Confirm the ability to search for specific text embeddings and verify the absence of replaced text in the updated vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /home/razvansavin/Proiecte/flexiai/examples/Code examples\n",
      "Changed Directory to: /home/razvansavin/Proiecte/flexiai\n",
      "Project root added to sys.path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current Directory: {current_dir}\")\n",
    "\n",
    "# Change to your project root directory\n",
    "project_root = '/home/razvansavin/Proiecte/flexiai'\n",
    "os.chdir(project_root)\n",
    "print(f\"Changed Directory to: {os.getcwd()}\")\n",
    "\n",
    "# Add project root directory to sys.path\n",
    "sys.path.append(project_root)\n",
    "print(f\"Project root added to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: faiss-cpu in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/razvansavin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install faiss-cpu\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n",
    "import faiss\n",
    "print(faiss.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing FlexiAI Client, Logging, and Setting Up Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/razvansavin/Proiecte/flexiai\n",
      "Log directory '/home/razvansavin/Proiecte/flexiai/logs' created/exists.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from flexiai.core.flexiai_client import FlexiAI\n",
    "from flexiai.config.logging_config import setup_logging\n",
    "\n",
    "# Initialize logging and FlexiAI client\n",
    "setup_logging(\n",
    "    root_level=logging.INFO, \n",
    "    file_level=logging.INFO, \n",
    "    console_level=logging.INFO, \n",
    "    enable_file_logging=True, \n",
    "    enable_console_logging=False\n",
    "    )\n",
    "flexiai = FlexiAI()\n",
    "logger = flexiai.logger\n",
    "local_vector_store_manager = flexiai.local_vector_store_manager\n",
    "\n",
    "# Define paths and initial parameters\n",
    "corpus_directory = 'user_flexiai_rag/data/corpus'\n",
    "vector_store_path = \"user_flexiai_rag/data/vectors_store/vector_store.index\"\n",
    "target_file = 'user_flexiai_rag/data/corpus/machine_learning.txt'\n",
    "query_text = 'I love cooking and traveling.'\n",
    "new_text = 'Machine learning is a subset of artificial intelligence focused on building systems that can learn from historical data, identify patterns, and make logical decisions with little to no human intervention.'\n",
    "save_path = \"user_flexiai_rag/data/vectors_store/updated_vector_store_after_replacement.index\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Read the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all text files from the corpus directory and extract their content and file paths.\n",
    "corpus = local_vector_store_manager.read_corpus_from_directory(corpus_directory)\n",
    "texts = [content for _, content in corpus]\n",
    "file_paths = [file_path for file_path, _ in corpus]\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create and Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each text and save them in a vector store.\n",
    "index, successful_texts = local_vector_store_manager.embedding_manager.create_embeddings_for_faiss(\n",
    "    texts, model=\"text-embedding-ada-002\", chunk_size=1000\n",
    ")\n",
    "metadata = {i: file_paths[i] for i in range(len(successful_texts))}\n",
    "local_vector_store_manager.save_vector_store(index, vector_store_path, metadata)\n",
    "print(100*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load and Map Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector 0: [-0.00994757  0.00371048  0.01932266 -0.01129005  0.00739952]...\n",
      "Vector 1: [-0.01725795  0.00876854  0.01848795  0.00226284  0.00612579]...\n",
      "Vector 2: [-0.01158029  0.00336338  0.0123597  -0.02616081 -0.0063383 ]...\n",
      "Vector 3: [-0.00664699 -0.00770192  0.00927453 -0.01479309 -0.00318138]...\n",
      "Vector 4: [ 5.0776620e-05 -6.4961184e-03  1.1508176e-02 -2.6941761e-02\n",
      " -1.2037743e-02]...\n",
      "Vector 5: [ 0.00940533 -0.00943127 -0.00508456 -0.01683684 -0.02007872]...\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the previously saved vector store and map it for future use.\n",
    "loaded_index, loaded_metadata = local_vector_store_manager.load_vector_store(vector_store_path)\n",
    "local_vector_store_manager.map_vector_store(loaded_index)\n",
    "print(100*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Text Similarity and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Text Similarity and Search ---\n",
      "Most similar text: https://en.wikipedia.org/wiki/Probability\n",
      "\n",
      "Probability is a numerical description of how likely an event is to occur or how likely it is that a proposition is true.  Probability is a number between 0 and 1, where, roughly  speaking, 0 indicates impossibility and 1 indicates certainty. The higher the...\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate the similarity between the input text and all texts in the corpus.\n",
    "input_text = \"The scientific study of probability is a modern development of mathematics.\"\n",
    "input_embedding = local_vector_store_manager.embedding_manager.create_embeddings(input_text)\n",
    "embeddings = [local_vector_store_manager.embedding_manager.create_embeddings(text) for text in texts]\n",
    "similarities = [np.dot(input_embedding, emb) / (np.linalg.norm(input_embedding) * np.linalg.norm(emb)) for emb in embeddings]\n",
    "most_similar_index = np.argmax(similarities)\n",
    "print(\"\\n--- Text Similarity and Search ---\")\n",
    "most_similar_text = texts[most_similar_index]\n",
    "# Truncate the output to 300 characters\n",
    "truncated_text = most_similar_text[:300] + '...' if len(most_similar_text) > 300 else most_similar_text\n",
    "print(f\"Most similar text: {truncated_text}\")\n",
    "print(100*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Clustering Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Clustering Texts ---\n",
      "====================================================================================================\n",
      "Cluster 0:\n",
      " - https://en.wikipedia.org/wiki/Neural_network\n",
      "\n",
      "Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being progr...\n",
      " - https://en.wikipedia.org/wiki/Machine_learning\n",
      "\n",
      "Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training ...\n",
      " - https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "\n",
      "In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as...\n",
      " - https://en.wikipedia.org/wiki/Probability\n",
      "\n",
      "Probability is a numerical description of how likely an event is to occur or how likely it is that a proposition is true.  Probability is a number between 0 and 1, where, roughly  speaking, 0 indicates impossibility and 1 indicates certainty. The higher the...\n",
      "====================================================================================================\n",
      "Cluster 1:\n",
      " - https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "\n",
      "evel, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-o...\n",
      "====================================================================================================\n",
      "Cluster 2:\n",
      " - https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to prog...\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cluster the texts into a specified number of clusters and print out each cluster.\n",
    "embeddings = [local_vector_store_manager.embedding_manager.create_embeddings(text) for text in texts]\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(scaled_embeddings)\n",
    "print(\"\\n--- Clustering Texts ---\")\n",
    "print(100*'=')\n",
    "for i in range(3):\n",
    "    cluster_texts = [texts[j] for j in range(len(texts)) if clusters[j] == i]\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for text in cluster_texts:\n",
    "        # Truncate each text to 300 characters\n",
    "        truncated_text = text[:300] + '...' if len(text) > 300 else text\n",
    "        print(f\" - {truncated_text}\")\n",
    "    print(100*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Semantic Search ---\n",
      "Most relevant document: https://en.wikipedia.org/wiki/Neural_network\n",
      "\n",
      "Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being progr...\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search for the query and find the most relevant document.\n",
    "query = \"How do neurons connect in a neural network?\"\n",
    "query_embedding = local_vector_store_manager.embedding_manager.create_embeddings(query)\n",
    "doc_embeddings = [local_vector_store_manager.embedding_manager.create_embeddings(doc) for doc in texts]\n",
    "similarities = [np.dot(query_embedding, emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb)) for emb in doc_embeddings]\n",
    "most_relevant_index = np.argmax(similarities)\n",
    "print(\"\\n--- Semantic Search ---\")\n",
    "most_relevant_document = texts[most_relevant_index]\n",
    "# Truncate the output to 300 characters\n",
    "truncated_document = most_relevant_document[:300] + '...' if len(most_relevant_document) > 300 else most_relevant_document\n",
    "print(f\"Most relevant document: {truncated_document}\")\n",
    "print(100*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/razvansavin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question Answering ---\n",
      "Answer: Types of supervised learning algorithms include Active learning , classification and regression.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# # Find the sentence in the context that is most similar to the question.\n",
    "# question = \"What are the types of supervised learning?\"\n",
    "# context_embeddings = [local_vector_store_manager.embedding_manager.create_embeddings(sentence) for sentence in texts]\n",
    "# question_embedding = local_vector_store_manager.embedding_manager.create_embeddings(question)\n",
    "# similarities = [np.dot(question_embedding, emb) / (np.linalg.norm(question_embedding) * np.linalg.norm(emb)) for emb in context_embeddings]\n",
    "# most_relevant_index = np.argmax(similarities)\n",
    "# print(\"\\n--- Question Answering ---\")\n",
    "# most_relevant_answer = texts[most_relevant_index]\n",
    "# # Truncate the output to 300 characters\n",
    "# truncated_answer = most_relevant_answer[:300] + '...' if len(most_relevant_answer) > 300 else most_relevant_answer\n",
    "# print(f\"Answer: {truncated_answer}\")\n",
    "# print(100*'=')\n",
    "\n",
    "\n",
    "\n",
    "# Define the question to be answered\n",
    "question = \"What are the types of supervised learning?\"\n",
    "\n",
    "# Step 1: Find the most relevant document in the corpus\n",
    "context_embeddings = [local_vector_store_manager.embedding_manager.create_embeddings(sentence) for sentence in texts]\n",
    "question_embedding = local_vector_store_manager.embedding_manager.create_embeddings(question)\n",
    "similarities = [np.dot(question_embedding, emb) / (np.linalg.norm(question_embedding) * np.linalg.norm(emb)) for emb in context_embeddings]\n",
    "most_relevant_doc_index = np.argmax(similarities)\n",
    "\n",
    "# Step 2: Extract the relevant document\n",
    "most_relevant_document = texts[most_relevant_doc_index]\n",
    "\n",
    "# Step 3: Split the document into sentences\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = nltk.sent_tokenize(most_relevant_document)\n",
    "\n",
    "# Step 4: Compute the similarity of each sentence with the question\n",
    "sentence_embeddings = [local_vector_store_manager.embedding_manager.create_embeddings(sentence) for sentence in sentences]\n",
    "sentence_similarities = [np.dot(question_embedding, emb) / (np.linalg.norm(question_embedding) * np.linalg.norm(emb)) for emb in sentence_embeddings]\n",
    "\n",
    "# Step 5: Find the sentence with the highest similarity score\n",
    "most_relevant_sentence_index = np.argmax(sentence_similarities)\n",
    "most_relevant_sentence = sentences[most_relevant_sentence_index]\n",
    "\n",
    "# Step 6: Truncate the output to 300 characters\n",
    "truncated_answer = most_relevant_sentence[:300] + '...' if len(most_relevant_sentence) > 300 else most_relevant_sentence\n",
    "\n",
    "# Print the answer\n",
    "print(\"\\n--- Question Answering ---\")\n",
    "print(f\"Answer: {truncated_answer}\")\n",
    "print(100 * '=')\n",
    "\n",
    "\n",
    "# If you want to build something similar and free, probably this project can help you: https://github.com/SavinRazvan/questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Replace Text and Update Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old sentence: I love cooking and traveling.\n",
      "Old embedding (first 5 elements): [-0.01158029  0.00336338  0.0123597  -0.02616081 -0.0063383 ]\n",
      "\n",
      "Similarity before replacement: 0.7280\n",
      "New sentence: Machine learning is a subset of artificial intelligence focused on building systems that can learn from historical data, identify patterns, and make logical decisions with little to no human intervention.\n",
      "New embedding (first 5 elements): [-0.03284407 -0.00081092  0.00623034 -0.01714912 -0.01093131]\n",
      "\n",
      "Similarity after replacement: 1.0000\n",
      "Updated vector store after replacing text in the file\n",
      "\n",
      "--- Replacing Text and Updating Vector Store ---\n",
      "Text replacement and vector store update completed.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Replace specific text in a target file and update the vector store accordingly.\n",
    "updated_index, new_embedding = local_vector_store_manager.replace_text_in_file_and_update_vector_store(\n",
    "    target_file, query_text, new_text, loaded_index, loaded_metadata, save_path\n",
    ")\n",
    "print(\"\\n--- Replacing Text and Updating Vector Store ---\")\n",
    "print(\"Text replacement and vector store update completed.\")\n",
    "print(100*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Search for Old Sentence in Updated Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search for Old Sentence in Updated Vector Store ---\n",
      "Old sentence not found in the vector store, replacement was successful.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Search for the old sentence in the updated vector store to ensure the replacement was successful.\n",
    "indices, distances = local_vector_store_manager.search_for_text_in_vector_store(query_text, updated_index)\n",
    "print(\"\\n--- Search for Old Sentence in Updated Vector Store ---\")\n",
    "if len(indices) > 0 and distances[0] > 0.9:  # Similarity threshold can be adjusted as needed\n",
    "    print(f\"Old sentence found in the vector store with similarity: {distances[0]:.4f}\")\n",
    "else:\n",
    "    print(\"Old sentence not found in the vector store, replacement was successful.\")\n",
    "print(100*'=')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
