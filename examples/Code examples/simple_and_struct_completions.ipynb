{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /home/razvansavin/Proiecte/flexiai/examples/Code examples\n",
      "Changed Directory to: /home/razvansavin/Proiecte/flexiai\n",
      "Project root added to sys.path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current Directory: {current_dir}\")\n",
    "\n",
    "# Change to your project root directory\n",
    "project_root = '/home/razvansavin/Proiecte/flexiai'\n",
    "os.chdir(project_root)\n",
    "print(f\"Changed Directory to: {os.getcwd()}\")\n",
    "\n",
    "# Add project root directory to sys.path\n",
    "sys.path.append(project_root)\n",
    "print(f\"Project root added to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/razvansavin/.conda_flexi/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:19,235 - INFO - task_manager.py - Initializing TaskManager\n",
      "2024-08-10 01:18:19,237 - INFO - function_mapping.py - User directory detected: /home/razvansavin/Proiecte/flexiai/user_flexiai_rag\n",
      "2024-08-10 01:18:19,238 - INFO - run_manager.py - Initialized RunManager with personal functions: []\n",
      "2024-08-10 01:18:19,239 - INFO - run_manager.py - Initialized RunManager with assistant functions: []\n",
      "2024-08-10 01:18:19,240 - INFO - function_mapping.py - Attempting to import module: user_task_manager\n",
      "2024-08-10 01:18:19,242 - INFO - function_mapping.py - Attempting to import module: user_helpers\n",
      "2024-08-10 01:18:19,243 - INFO - function_mapping.py - Attempting to import module: user_function_mapping\n",
      "2024-08-10 01:18:19,244 - INFO - user_function_mapping.py - Registering user tasks\n",
      "2024-08-10 01:18:19,244 - INFO - function_mapping.py - Successfully registered user functions from user_function_mapping\n",
      "2024-08-10 01:18:19,246 - INFO - task_manager.py - User-defined tasks loaded successfully\n",
      "2024-08-10 01:18:19,247 - INFO - task_manager.py - Assistant function mapping: {'communicate_with_assistant': <bound method UserTaskManager.continue_conversation_with_assistant of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5c87f9a250>>}\n",
      "2024-08-10 01:18:19,248 - INFO - task_manager.py - Personal function mapping: {'save_processed_content': <bound method UserTaskManager.save_processed_content of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5c87f9a250>>, 'load_processed_content': <bound method UserTaskManager.load_processed_content of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5c87f9a250>>, 'initialize_agent': <bound method UserTaskManager.initialize_agent of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5c87f9a250>>, 'search_youtube': <bound method UserTaskManager.search_youtube of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5c87f9a250>>}\n",
      "2024-08-10 01:18:19,249 - INFO - run_manager.py - Initialized RunManager with personal functions: ['save_processed_content', 'load_processed_content', 'initialize_agent', 'search_youtube']\n",
      "2024-08-10 01:18:19,250 - INFO - run_manager.py - Initialized RunManager with assistant functions: ['communicate_with_assistant']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/razvansavin/Proiecte/flexiai\n",
      "Log directory '/home/razvansavin/Proiecte/flexiai/logs' created/exists.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "from flexiai.core.flexiai_client import FlexiAI\n",
    "from flexiai.config.logging_config import setup_logging\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# Initialize logging and FlexiAI client\n",
    "setup_logging(\n",
    "    root_level=logging.INFO, \n",
    "    file_level=logging.INFO, \n",
    "    console_level=logging.INFO, \n",
    "    enable_file_logging=True, \n",
    "    enable_console_logging=True\n",
    "    )\n",
    "flexiai = FlexiAI()\n",
    "logger = flexiai.logger\n",
    "completions_manager = flexiai.completions_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Test Simple Chat Completion\n",
    "> This will test the basic completion functionality without any structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:19,259 - INFO - completions_manager.py - Performing simple chat completion with model gpt-4o-mini\n",
      "2024-08-10 01:18:20,494 - INFO - _client.py - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-10 01:18:20,500 - INFO - completions_manager.py - Received message content: I'm sorry, but I can't provide real-time weather updates. However, you can check a reliable weather website or app for the latest information on today's weather. If you tell me your location, I can guide you on where to find the weather details!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Completion Result: I'm sorry, but I can't provide real-time weather updates. However, you can check a reliable weather website or app for the latest information on today's weather. If you tell me your location, I can guide you on where to find the weather details!\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt for testing\n",
    "simple_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like today?\"}\n",
    "]\n",
    "\n",
    "# Perform the simple chat completion\n",
    "simple_result = completions_manager.simple_chat_completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=simple_messages\n",
    ")\n",
    "\n",
    "# Output the result\n",
    "print(\"Simple Completion Result:\", simple_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test Structured Chat Completion\n",
    "> This will test the structured output functionality using a predefined schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"type\": \"object\",\n",
    "#   \"required\": [\"steps\", \"final_answer\"],\n",
    "#   \"properties\": {\n",
    "#     \"steps\": {\n",
    "#       \"type\": \"array\",\n",
    "#       \"items\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"required\": [\"explanation\", \"output\"],\n",
    "#         \"properties\": {\n",
    "#           \"output\": {\"type\": \"string\"},\n",
    "#           \"explanation\": {\"type\": \"string\"}\n",
    "#         },\n",
    "#         \"additionalProperties\": false\n",
    "#       }\n",
    "#     },\n",
    "#     \"final_answer\": {\"type\": \"string\"}\n",
    "#   },\n",
    "#   \"additionalProperties\": false\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:20,521 - INFO - completions_manager.py - Performing structured chat completion with model gpt-4o-mini\n",
      "2024-08-10 01:18:24,448 - INFO - _client.py - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-10 01:18:24,454 - INFO - completions_manager.py - Received structured output: {\"steps\":[{\"output\":\"Add all the numbers together: 4 + 8 + 6 + 5 + 3\",\"explanation\":\"To find the average, you first need to find the sum of all the numbers.\"},{\"output\":\"4 + 8 + 6 + 5 + 3 = 26\",\"explanation\":\"When you perform the addition, the total sum of the numbers is 26.\"},{\"output\":\"Count how many numbers there are: 5\",\"explanation\":\"Next, you count how many numbers you are averaging. In this case, there are 5 numbers.\"},{\"output\":\"Divide the sum by the count: 26 / 5\",\"explanation\":\"To find the average, you divide the total sum by the number of values.\"},{\"output\":\"26 / 5 = 5.2\",\"explanation\":\"When you divide 26 by 5, you get 5.2.\"}],\"final_answer\":\"The average of the numbers 4, 8, 6, 5, and 3 is 5.2.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=\n",
      "Structured Completion Result:\n",
      "Final Answer: The average of the numbers 4, 8, 6, 5, and 3 is 5.2.\n",
      "\n",
      "Steps:\n",
      "Explanation: To find the average, you first need to find the sum of all the numbers.\n",
      "Output: Add all the numbers together: 4 + 8 + 6 + 5 + 3\n",
      "\n",
      "Explanation: When you perform the addition, the total sum of the numbers is 26.\n",
      "Output: 4 + 8 + 6 + 5 + 3 = 26\n",
      "\n",
      "Explanation: Next, you count how many numbers you are averaging. In this case, there are 5 numbers.\n",
      "Output: Count how many numbers there are: 5\n",
      "\n",
      "Explanation: To find the average, you divide the total sum by the number of values.\n",
      "Output: Divide the sum by the count: 26 / 5\n",
      "\n",
      "Explanation: When you divide 26 by 5, you get 5.2.\n",
      "Output: 26 / 5 = 5.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the structured output\n",
    "math_response_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"steps\", \"final_answer\"],\n",
    "    \"properties\": {\n",
    "        \"steps\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"required\": [\"explanation\", \"output\"],\n",
    "                \"properties\": {\n",
    "                    \"output\": {\"type\": \"string\"},\n",
    "                    \"explanation\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"final_answer\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# Define the messages\n",
    "structured_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a math assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain how to calculate the average of the numbers 4, 8, 6, 5, and 3.\"}\n",
    "]\n",
    "\n",
    "# Perform the structured chat completion\n",
    "structured_result = completions_manager.structured_chat_completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=structured_messages,\n",
    "    schema_name=\"math_response\",\n",
    "    schema=math_response_schema\n",
    ")\n",
    "\n",
    "# Parse the JSON string into a Python dictionary\n",
    "structured_output = json.loads(structured_result)\n",
    "print(*'=')\n",
    "# Print the output in a readable format\n",
    "print(\"Structured Completion Result:\")\n",
    "print(f\"Final Answer: {structured_output['final_answer']}\\n\")\n",
    "\n",
    "print(\"Steps:\")\n",
    "for step in structured_output['steps']:\n",
    "    print(f\"Explanation: {step['explanation']}\")\n",
    "    print(f\"Output: {step['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test Function Calling Completion\n",
    "\n",
    "> This will test the function calling capability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:24,484 - INFO - task_manager.py - Initializing TaskManager\n",
      "2024-08-10 01:18:24,527 - INFO - function_mapping.py - User directory detected: /home/razvansavin/Proiecte/flexiai/user_flexiai_rag\n",
      "2024-08-10 01:18:24,527 - INFO - run_manager.py - Initialized RunManager with personal functions: []\n",
      "2024-08-10 01:18:24,529 - INFO - run_manager.py - Initialized RunManager with assistant functions: []\n",
      "2024-08-10 01:18:24,530 - INFO - function_mapping.py - Attempting to import module: user_task_manager\n",
      "2024-08-10 01:18:24,532 - INFO - function_mapping.py - Attempting to import module: user_helpers\n",
      "2024-08-10 01:18:24,533 - INFO - function_mapping.py - Attempting to import module: user_function_mapping\n",
      "2024-08-10 01:18:24,534 - INFO - user_function_mapping.py - Registering user tasks\n",
      "2024-08-10 01:18:24,535 - INFO - function_mapping.py - Successfully registered user functions from user_function_mapping\n",
      "2024-08-10 01:18:24,536 - INFO - task_manager.py - User-defined tasks loaded successfully\n",
      "2024-08-10 01:18:24,537 - INFO - task_manager.py - Assistant function mapping: {'communicate_with_assistant': <bound method UserTaskManager.continue_conversation_with_assistant of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5ca8efb390>>}\n",
      "2024-08-10 01:18:24,538 - INFO - task_manager.py - Personal function mapping: {'save_processed_content': <bound method UserTaskManager.save_processed_content of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5ca8efb390>>, 'load_processed_content': <bound method UserTaskManager.load_processed_content of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5ca8efb390>>, 'initialize_agent': <bound method UserTaskManager.initialize_agent of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5ca8efb390>>, 'search_youtube': <bound method UserTaskManager.search_youtube of <user_flexiai_rag.user_task_manager.UserTaskManager object at 0x7f5ca8efb390>>}\n",
      "2024-08-10 01:18:24,539 - INFO - run_manager.py - Initialized RunManager with personal functions: ['save_processed_content', 'load_processed_content', 'initialize_agent', 'search_youtube']\n",
      "2024-08-10 01:18:24,539 - INFO - run_manager.py - Initialized RunManager with assistant functions: ['communicate_with_assistant']\n",
      "2024-08-10 01:18:24,541 - INFO - completions_manager.py - Performing function calling completion with model gpt-4o-mini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/razvansavin/Proiecte/flexiai\n",
      "Log directory '/home/razvansavin/Proiecte/flexiai/logs' created/exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:26,573 - INFO - _client.py - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-10 01:18:26,576 - INFO - completions_manager.py - Function call made: FunctionCall(arguments='{\"location\":\"New York\",\"unit\":\"C\"}', name='get_weather')\n",
      "2024-08-10 01:18:26,576 - INFO - completions_manager.py - Received message content: None\n",
      "2024-08-10 01:18:26,577 - INFO - completions_manager.py - Performing simple chat completion with model gpt-4o-mini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Call Detected:\n",
      "FunctionCall(arguments='{\"location\":\"New York\",\"unit\":\"C\"}', name='get_weather')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:27,624 - INFO - _client.py - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-10 01:18:27,625 - INFO - completions_manager.py - Received message content: I'm unable to provide real-time weather information. However, you can easily check the current weather in New York by using a weather website or app. Typically, New York experiences a range of temperatures depending on the season, so be sure to check for up-to-date information!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow-Up Result: I'm unable to provide real-time weather information. However, you can easily check the current weather in New York by using a weather website or app. Typically, New York experiences a range of temperatures depending on the season, so be sure to check for up-to-date information!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from flexiai.core.flexiai_client import FlexiAI\n",
    "from flexiai.config.logging_config import setup_logging\n",
    "\n",
    "\n",
    "def initialize_flexiai():\n",
    "    \"\"\"Initializes FlexiAI client and logger.\"\"\"\n",
    "    setup_logging()\n",
    "    flexiai = FlexiAI()\n",
    "    logger = flexiai.logger\n",
    "    return flexiai, logger\n",
    "\n",
    "\n",
    "def check_function_call(function_call):\n",
    "    \"\"\"Simulates the execution of a function if a function call is detected.\"\"\"\n",
    "    if function_call:\n",
    "        print(\"Function Call Detected:\")\n",
    "        print(function_call)\n",
    "\n",
    "        # Simulate function execution result\n",
    "        simulated_function_result = {\n",
    "            \"location\": \"New York\",\n",
    "            \"temperature\": \"15°C\",\n",
    "            \"condition\": \"Partly cloudy\"\n",
    "        }\n",
    "\n",
    "        return str(simulated_function_result)\n",
    "    else:\n",
    "        print(\"No function call detected.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def handle_follow_up(completions_manager, model, messages, function_call_result):\n",
    "    \"\"\"Handles the follow-up interaction after simulating the function execution.\"\"\"\n",
    "    if function_call_result:\n",
    "        follow_up_messages = messages + [{\"role\": \"assistant\", \"content\": function_call_result}]\n",
    "        follow_up_result = completions_manager.simple_chat_completion(model=model, messages=follow_up_messages)\n",
    "        print(\"Follow-Up Result:\", follow_up_result)\n",
    "    else:\n",
    "        print(\"Initial Result:\", function_call_result)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize FlexiAI and get the logger and manager\n",
    "    flexiai, logger = initialize_flexiai()\n",
    "    completions_manager = flexiai.completions_manager\n",
    "\n",
    "    # Define the function that the model can call\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Fetches the weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"C\", \"F\"]}\n",
    "                },\n",
    "                \"required\": [\"location\", \"unit\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the messages\n",
    "    function_calling_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that can fetch weather information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Get the weather in New York in Celsius.\"}\n",
    "    ]\n",
    "\n",
    "    # Perform the function calling chat completion\n",
    "    function_call_result, function_call = completions_manager.function_calling_completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=function_calling_messages,\n",
    "        functions=functions\n",
    "    )\n",
    "\n",
    "    # Check and handle the function call\n",
    "    simulated_result = check_function_call(function_call)\n",
    "\n",
    "    # Handle follow-up interaction\n",
    "    handle_follow_up(completions_manager, \"gpt-4o-mini\", function_calling_messages, simulated_result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test Refusal Handling\n",
    "\n",
    "> This test checks how the model handles scenarios where it refuses to fulfill a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:55,755 - INFO - completions_manager.py - Performing simple chat completion with model gpt-4o-mini\n",
      "2024-08-10 01:18:56,364 - INFO - _client.py - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-10 01:18:56,366 - INFO - completions_manager.py - Received message content: I’m sorry, I can’t assist with that.\n",
      "2024-08-10 01:18:56,366 - WARNING - completions_manager.py - Received string response: I’m sorry, I can’t assist with that.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refusal Reason: Potential refusal or error in response\n"
     ]
    }
   ],
   "source": [
    "# Define a message that might cause a refusal\n",
    "refusal_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a strict assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me how to hack a website.\"}\n",
    "]\n",
    "\n",
    "# Perform the structured chat completion\n",
    "refusal_result = completions_manager.simple_chat_completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=refusal_messages\n",
    ")\n",
    "\n",
    "# Handle and print the refusal\n",
    "refusal_reason = completions_manager.handle_refusals(refusal_result)\n",
    "print(\"Refusal Reason:\", refusal_reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:18:30,170 - INFO - _client.py - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from API:\n",
      "{\n",
      "  \"green tree frog\": {\n",
      "    \"color\": \"Bright green\",\n",
      "    \"temperament\": \"Calm\",\n",
      "    \"taste\": \"Mildly sweet\"\n",
      "  },\n",
      "  \"poison arrow frog\": {\n",
      "    \"color\": \"Vibrant blue\",\n",
      "    \"temperament\": \"Aggressive\",\n",
      "    \"taste\": \"Toxic\"\n",
      "  },\n",
      "  \"peeper\": {\n",
      "    \"color\": \"Brownish green\",\n",
      "    \"temperament\": \"Vocal\",\n",
      "    \"taste\": \"Unremarkable\"\n",
      "  },\n",
      "  \"bullfrog\": {\n",
      "    \"color\": \"Olive green\",\n",
      "    \"temperament\": \"Loud\",\n",
      "    \"taste\": \"Savory\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "def make_parameters(frogs: list[str]):\n",
    "    assert len(frogs) <= 25, \"OpenAI can only handle 25 frogs at once with this schema\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a biologist specializing in aquatics who has been tasked with describing frogs.\"},\n",
    "        {\"role\": \"user\", \"content\": \"For each frog, please describe its color, temperament, and taste in two words or less. Reply in JSON.\"},\n",
    "    ]\n",
    "    json_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            v: {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"color\": {\"type\": \"string\"},\n",
    "                    \"temperament\": {\"type\": \"string\"},\n",
    "                    \"taste\": {\"type\": \"string\"},\n",
    "                },\n",
    "                \"required\": [\"color\",\"temperament\",\"taste\"],\n",
    "                \"additionalProperties\": False,\n",
    "            }\n",
    "            for v in frogs\n",
    "        },\n",
    "        \"required\": frogs,\n",
    "        \"additionalProperties\": False,\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.1,\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"summarization\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": json_schema,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "client = OpenAI()\n",
    "parameters = make_parameters([\"green tree frog\", \"poison arrow frog\", \"peeper\", \"bullfrog\"])\n",
    "\n",
    "encoder = tiktoken.encoding_for_model(parameters[\"model\"])\n",
    "answer = client.chat.completions.create(**parameters)\n",
    "\n",
    "message = answer.choices[0].message.content\n",
    "local_output_tokens = len(encoder.encode(message))\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(\"Result from API:\")\n",
    "print(json.dumps(json.loads(message), indent=2))  # Pretty print the JSON response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
